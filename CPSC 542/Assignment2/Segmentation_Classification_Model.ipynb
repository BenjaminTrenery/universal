{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorrt as trt\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "from zipfile import ZipFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./augmented-forest-segmentation\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "\n",
    "import opendatasets as od\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/quadeer15sh/augmented-forest-segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation classification model class\n",
    "\n",
    "class SegmentationClassificationModel: \n",
    "    \n",
    "    #initializes object with file path to the csv file, number of classes, folder to the dataset, csv image column, csv mask column, image height, image width, and image channels\n",
    "    def __init__(self, datasetCSVFilePath, numClasses, datasetFolderPath, X_col, y_col, img_height, img_width, img_channels):\n",
    "        self.datasetCSVFilePath = datasetCSVFilePath\n",
    "        self.datasetFolderPath = datasetFolderPath\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.img_channels = img_channels\n",
    "        self.numClasses = numClasses\n",
    "        \n",
    "    # \n",
    "    def getDataset(self):\n",
    "        \n",
    "        #put csv in dataframe\n",
    "        myDatasetCSV = pd.read_csv(self.datasetCSVFilePath)\n",
    "        \n",
    "        # create two numpy arrays to put the image and mask\n",
    "        self.X = np.zeros((len(myDatasetCSV), self.img_height, self.img_width, self.img_channels), dtype = np.uint8)\n",
    "        self.y = np.zeros((len(myDatasetCSV), self.img_height, self.img_width, 1), dtype = bool)\n",
    "        i = 0\n",
    "        \n",
    "        # for each index and row in the csv file, read the image into a numpy array, resize it, and put it in X numpy array\n",
    "        for index, row in myDatasetCSV.iterrows():\n",
    "            \n",
    "            imagePath = self.datasetFolderPath + \"/images/\" + row[self.X_col]\n",
    "            myImage = imread(imagePath)[:,:,:self.img_channels]  \n",
    "            myImage = resize(myImage, (self.img_height, self.img_width), mode='constant', preserve_range=True)\n",
    "            self.X[i] = myImage\n",
    "            \n",
    "            # Load mask \n",
    "            maskDir = os.path.join(self.datasetFolderPath, \"masks\")\n",
    "            maskArray = np.zeros((self.img_height, self.img_width, 1), dtype=bool)\n",
    "            for mask_file in os.listdir(maskDir):\n",
    "                if mask_file.startswith(row[self.y_col]):\n",
    "                    maskPath = os.path.join(maskDir, mask_file)\n",
    "                    myMask = imread(maskPath)[:, :, 0]  # Assuming grayscale\n",
    "                    myMask = np.expand_dims(resize(myMask, (self.img_height, self.img_width), mode='constant',  \n",
    "                                        preserve_range=True), axis=-1)\n",
    "                    maskArray = np.maximum(maskArray, myMask)\n",
    "        \n",
    "        self.y[i] = maskArray\n",
    "        i += 1\n",
    "    \n",
    "    #split dataset into train and test \n",
    "    def trainTestSplit(self):\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size = 0.2)\n",
    "    \n",
    "    # divide by 255 for relu activate do that the pixel range is from 0 - 1 rather than 0 - 255\n",
    "    def preProcessing(self):\n",
    "        \n",
    "        self.X_train = self.X_train / 255.0\n",
    "        self.X_test = self.X_test / 255.0\n",
    "    \n",
    "    # \n",
    "    def trainValSplit(self):\n",
    "        \n",
    "        self.X_train, X_val, self.y_train, y_val = train_test_split(self.X_train, self.y_train, test_size = 0.2)\n",
    "        \n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "    \n",
    "    # was used for the cnn model to display images of the \n",
    "    # def displayClassImages(self):\n",
    "        \n",
    "    #     plt.figure(figsize=(10,10))\n",
    "    #     for i in range(6):\n",
    "    #         plt.subplot(5,5,i+1)\n",
    "    #         plt.xticks([])\n",
    "    #         plt.yticks([])\n",
    "    #         plt.grid(False)\n",
    "    #         plt.imshow(self.X_train[i])\n",
    "    #         plt.xlabel(self.y_train[i])\n",
    "    #     plt.show()\n",
    "            \n",
    "    def augmentDataset(self, h_flip, v_flip, rot_range, myZoom_range):\n",
    "        \n",
    "        image_generator = ImageDataGenerator(\n",
    "            horizontal_flip = h_flip,\n",
    "            vertical_flip = v_flip,\n",
    "            rotation_range = rot_range,\n",
    "            zoom_range = myZoom_range)\n",
    "        \n",
    "        self.train_generator = image_generator.flow(self.X_train, self.y_train)\n",
    "    \n",
    "    # cnn model function used for testing earlier\n",
    "    \n",
    "    # def cnnModel(self, numConvolutions, numDropout, dropoutRate, isBatchNormalization):\n",
    "        \n",
    "    #     myNumDropout = numDropout\n",
    "        \n",
    "    #     self.model = models.Sequential()\n",
    "    #     self.model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(self.img_height, self.img_width, self.img_channels)))\n",
    "    #     self.model.add(layers.MaxPooling2D((2, 2)))\n",
    "        \n",
    "    #     for i in range(0, numConvolutions):\n",
    "            \n",
    "    #         self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    #         self.model.add(layers.MaxPooling2D((2, 2)))\n",
    "            \n",
    "    #         if(myNumDropout > 0):\n",
    "                \n",
    "    #             self.model.add(Dropout(dropoutRate))\n",
    "    #             myNumDropout -= 1\n",
    "                \n",
    "    #         if(isBatchNormalization == True):\n",
    "                \n",
    "    #             self.model.add(BatchNormalization())\n",
    "        \n",
    "    #     self.model.add(layers.Flatten())\n",
    "    #     self.model.add(layers.Dense(64, activation='relu'))\n",
    "    #     self.model.add(layers.Dense(self.num_classes))\n",
    "    \n",
    "    # u-net model with a dropout rate input, contracts and then expands the \n",
    "    def uNetModel(self, dropoutRate):\n",
    "        \n",
    "        self.inputs = tf.keras.layers.Input((self.img_height, self.img_width, self.img_channels))\n",
    "        \n",
    "        #Contraction path\n",
    "        conv1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(self.inputs)\n",
    "        conv1 = tf.keras.layers.Dropout(dropoutRate)(conv1)\n",
    "        conv1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv1)\n",
    "        batch1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "        relu1 = tf.keras.layers.ReLU()(batch1)\n",
    "        pool1 = tf.keras.layers.MaxPooling2D((2, 2))(relu1)\n",
    "\n",
    "        conv2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool1)\n",
    "        conv2 = tf.keras.layers.Dropout(dropoutRate)(conv2)\n",
    "        conv2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv2)\n",
    "        batch2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "        relu2 = tf.keras.layers.ReLU()(batch2)\n",
    "        pool2 = tf.keras.layers.MaxPooling2D((2, 2))(relu2)\n",
    "        \n",
    "        conv3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool2)\n",
    "        conv3 = tf.keras.layers.Dropout(dropoutRate)(conv3)\n",
    "        conv3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv3)\n",
    "        batch3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "        relu3 = tf.keras.layers.ReLU()(batch3)\n",
    "        pool3 = tf.keras.layers.MaxPooling2D((2, 2))(relu3)\n",
    "        \n",
    "        conv4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool3)\n",
    "        conv4 = tf.keras.layers.Dropout(dropoutRate)(conv4)\n",
    "        conv4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv4)\n",
    "        batch4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "        relu4 = tf.keras.layers.ReLU()(batch4)\n",
    "        pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(relu4)\n",
    "        \n",
    "        conv5 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool4)\n",
    "        batch5 = tf.keras.layers.BatchNormalization()(conv5)\n",
    "        relu5 = tf.keras.layers.ReLU()(batch5)\n",
    "        conv5 = tf.keras.layers.Dropout(dropoutRate)(relu5)\n",
    "        conv5 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv5)\n",
    "\n",
    "        # Expansive path \n",
    "        up1 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv5)\n",
    "        up1 = tf.keras.layers.concatenate([up1, conv4])\n",
    "        up1 = tf.keras.layers.BatchNormalization()(up1)\n",
    "        up1 = tf.keras.layers.ReLU()(up1)\n",
    "\n",
    "        \n",
    "        up2 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(up1)\n",
    "        up2 = tf.keras.layers.concatenate([up2, conv3])\n",
    "        up2 = tf.keras.layers.BatchNormalization()(up2)\n",
    "        up2 = tf.keras.layers.ReLU()(up2)\n",
    "\n",
    "        \n",
    "        up3 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(up2)\n",
    "        up3 = tf.keras.layers.concatenate([up3, conv2])\n",
    "        up3 = tf.keras.layers.BatchNormalization()(up3)\n",
    "        up3 = tf.keras.layers.ReLU()(up3)\n",
    "        \n",
    "        up4 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(up3)\n",
    "        up4 = tf.keras.layers.concatenate([up4, conv1], axis=3)\n",
    "        up4 = tf.keras.layers.BatchNormalization()(up4)\n",
    "        up4 = tf.keras.layers.ReLU()(up4)\n",
    "        \n",
    "        self.outputs = tf.keras.layers.Conv2D(self.numClasses, (1, 1), activation='sigmoid')(up4)\n",
    "\n",
    "        \n",
    "    # compiles and fits the cnn model (adam optimizer)\n",
    "    # def compileAndFitCNNModel(self, learningRate, numEpochs):\n",
    "        \n",
    "    #     optimizer = tf.keras.optimizers.Adam(learning_rate = learningRate)\n",
    "        \n",
    "        \n",
    "    #     self.model.compile(optimizer= optimizer,\n",
    "    #           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)),\n",
    "    \n",
    "    #           metrics=['accuracy'])\n",
    "\n",
    "    #     self.history = self.model.fit(self.train_generator, epochs=numEpochs,\n",
    "    #                         validation_data=(self.X_val, self.y_val))\n",
    "        \n",
    "    #compiles and fits u-net model (adam optimizer) (binary crossentropy)\n",
    "    def compileAndFitUNetModel(self, learningRate, numEpochs, batchSize):\n",
    "        \n",
    "        self.model = tf.keras.Model(inputs=[self.inputs], outputs=[self.outputs])\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = learningRate)\n",
    "        \n",
    "        self.model.compile(optimizer= optimizer,\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "        self.history = self.model.fit(self.train_generator, epochs=numEpochs, batch_size = batchSize,\n",
    "                            validation_data=(self.X_val, self.y_val)) \n",
    "    \n",
    "    def showModelLoss(self):\n",
    "        \n",
    "        #graph of the loss function between the training data and the validiation data\n",
    "\n",
    "        plt.figure(figsize=(15, 16))\n",
    "\n",
    "        plt.subplot(4, 2, 2)\n",
    "        plt.plot(self.history.history['loss'], label='Loss')\n",
    "        plt.plot(self.history.history['val_loss'], label='val_Loss')\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        \n",
    "    def showModelAccuracy(self):    \n",
    "        \n",
    "        #graph of the accuracy between the training data and the validiation data\n",
    "\n",
    "        plt.figure(figsize=(15, 16))\n",
    "\n",
    "        plt.subplot(4, 2, 2)\n",
    "        plt.plot(self.history.history['accuracy'], label='accuracy')\n",
    "        plt.plot(self.history.history['val_accuracy'], label='val_accuracy')\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "    \n",
    "\n",
    "    def showTable(self):\n",
    "        \n",
    "        # Extract accuracy, loss, validation accuracy, and validation loss values from the history object\n",
    "        history_dict = self.history.history\n",
    "        history_dict['epoch'] = range(1, len(history_dict['accuracy']) + 1)\n",
    "\n",
    "        # Convert the history dictionary to a DataFrame\n",
    "        self.historyDF = pd.DataFrame(history_dict)\n",
    "        \n",
    "        for key in history_dict.keys():\n",
    "            if key != 'epoch':\n",
    "                history_dict[key] = [round(value, 3) for value in history_dict[key]]\n",
    "\n",
    "        # Create a Matplotlib figure and axis\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "        # Hide axes\n",
    "        ax.axis('off')\n",
    "        plt.title(\"Accuracy and Loss over Epochs\")\n",
    "\n",
    "        # Create the table\n",
    "        ax.table(cellText=self.historyDF.values,\n",
    "                 colLabels=self.historyDF.columns,\n",
    "                 cellLoc='center',\n",
    "                 loc='center')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
